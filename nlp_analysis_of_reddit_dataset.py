# -*- coding: utf-8 -*-
"""NLP Analysis of Reddit Dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KdpkKRQc875oeg2Gzx-sSw2zjU-qEQNI

# ðŸ“Œ Packages
"""

from google.colab import drive
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
from wordcloud import WordCloud
from collections import Counter
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertForSequenceClassification
from sklearn.model_selection import train_test_split

"""# ðŸ“Œ Mount Google Drive"""

# Mount drive to access dataset
drive.mount('/content/drive')

# Define the path to your dataset (update if needed)
file_path = "/content/drive/My Drive/kaggle_RC_2019-05.csv"

"""# ðŸ“Œ Load and Explore Dataset"""

df = pd.read_csv(file_path)

# Display first few rows
df.head()

# Check dataset info
df.info()

# Drop any duplicate or null values
df.dropna(inplace=True)

"""# ðŸ“Œ Exploratory Data Analysis (EDA)"""

# Count number of posts per subreddit
plt.figure(figsize=(12,5))
sns.countplot(y=df['subreddit'], order=df['subreddit'].value_counts().index[:15])
plt.title("Top 15 Subreddits with Most Posts")
plt.xlabel("Number of Posts")
plt.ylabel("Subreddit")
plt.show()

# Text Length Distribution
df['text_length'] = df['body'].apply(lambda x: len(str(x).split()))
plt.figure(figsize=(10, 5))
sns.histplot(df['text_length'], bins=50, kde=True)
plt.title("Distribution of Comment Text Lengths")
plt.xlabel("Number of Words")
plt.ylabel("Frequency")
plt.show()

"""# ðŸ“Œ Sentiment Analysis using VADER"""

nltk.download('vader_lexicon')
sia = SentimentIntensityAnalyzer()

# Compute sentiment scores
df['sentiment_score'] = df['body'].apply(lambda x: sia.polarity_scores(str(x))['compound'])

# Categorize into Positive, Neutral, Negative
df['sentiment'] = df['sentiment_score'].apply(lambda x: 'Positive' if x > 0.05 else ('Negative' if x < -0.05 else 'Neutral'))

# Sentiment Distribution
plt.figure(figsize=(8,5))
sns.countplot(x=df['sentiment'], palette="coolwarm")
plt.title("Sentiment Distribution of Reddit Comments")
plt.xlabel("Sentiment")
plt.ylabel("Count")
plt.show()

"""# ðŸ“Œ Word Cloud for Most Common Words"""

all_text = " ".join(df['body'].dropna())
wordcloud = WordCloud(width=800, height=400, background_color='black', colormap='cool').generate(all_text)

plt.figure(figsize=(12,6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.title("Most Common Words in Reddit Comments")
plt.show()

"""# ðŸ“Œ NLP Model - BERT for Sentiment Classification"""

import nltk
from nltk.sentiment import SentimentIntensityAnalyzer

# Download VADER sentiment lexicon
nltk.download('vader_lexicon')
sia = SentimentIntensityAnalyzer()

# Apply sentiment analysis
df['sentiment_score'] = df['body'].apply(lambda x: sia.polarity_scores(str(x))['compound'])

# Define sentiment categories
def categorize_sentiment(score):
    if score > 0.05:
        return 'Positive'
    elif score < -0.05:
        return 'Negative'
    else:
        return 'Neutral'

df['sentiment'] = df['sentiment_score'].apply(categorize_sentiment)

# Display first few rows to confirm
df[['body', 'sentiment']].head()

print(df.columns)  # Ensure 'sentiment' exists
print(df['sentiment'].value_counts())  # Check sentiment distribution

sentiment_mapping = {'Negative': 0, 'Neutral': 1, 'Positive': 2}
df['label'] = df['sentiment'].map(sentiment_mapping)

# Verify mapping
df[['sentiment', 'label']].head()

# Load BERT tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

# Tokenize comments
df['tokenized'] = df['body'].apply(lambda x: tokenizer.encode(str(x), truncation=True, padding='max_length', max_length=128))

# Convert Sentiment to Labels
sentiment_mapping = {'Negative': 0, 'Neutral': 1, 'Positive': 2}
df['label'] = df['sentiment'].map(sentiment_mapping)

# Split data
train_texts, test_texts, train_labels, test_labels = train_test_split(df['tokenized'].tolist(), df['label'].tolist(), test_size=0.1, random_state=42)

# Convert to PyTorch tensors
train_texts, test_texts = torch.tensor(train_texts), torch.tensor(test_texts)
train_labels, test_labels = torch.tensor(train_labels), torch.tensor(test_labels)

# Create PyTorch dataset
class RedditDataset(Dataset):
    def __init__(self, texts, labels):
        self.texts = texts
        self.labels = labels

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        return self.texts[idx], self.labels[idx]

train_dataset = RedditDataset(train_texts, train_labels)
test_dataset = RedditDataset(test_texts, test_labels)

# Create DataLoader
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)

# Load BERT model
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=3)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

"""# ðŸ“Œ Train a Simple BERT Model"""

from transformers import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)

# Training loop
for epoch in range(3):
    model.train()
    total_loss = 0
    for batch in train_loader:
        inputs, labels = batch
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(inputs, labels=labels)
        loss = outputs.loss
        total_loss += loss.item()
        loss.backward()
        optimizer.step()

    print(f"Epoch {epoch+1}, Loss: {total_loss/len(train_loader)}")

"""# ðŸ“Œ Evaluate Model Performance"""

from sklearn.metrics import classification_report

model.eval()
predictions, true_labels = [], []

with torch.no_grad():
    for batch in test_loader:
        inputs, labels = batch
        inputs, labels = inputs.to(device), labels.to(device)
        outputs = model(inputs)
        preds = torch.argmax(outputs.logits, axis=1)
        predictions.extend(preds.cpu().numpy())
        true_labels.extend(labels.cpu().numpy())

print(classification_report(true_labels, predictions, target_names=["Negative", "Neutral", "Positive"]))

"""# ðŸ“Œ Text Preprocessing"""

import string

df = pd.read_csv(file_path)

# Display first few rows
df = df[['subreddit', 'body']].dropna()
df.head()

nltk.download('stopwords')
from nltk.corpus import stopwords

# Function to clean text
def preprocess_text(text):
    text = text.lower()  # Convert to lowercase
    text = re.sub(r'\[.*?\]', '', text)  # Remove brackets
    text = re.sub(r'\w*\d\w*', '', text)  # Remove numbers
    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation
    text = " ".join([word for word in text.split() if word not in stopwords.words('english')])  # Remove stopwords
    return text

# Apply preprocessing
df['clean_text'] = df['body'].apply(preprocess_text)

"""# ðŸ“Œ TF-IDF Vectorization"""

# ================================
# ðŸ“Œ STEP 3: TF-IDF Vectorization
# ================================
vectorizer = TfidfVectorizer(max_features=100)
X_tfidf = vectorizer.fit_transform(df['clean_text'])

# Convert to DataFrame
tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=vectorizer.get_feature_names_out())

# Plot Top TF-IDF Features
tfidf_mean = tfidf_df.mean().sort_values(ascending=False)[:20]
plt.figure(figsize=(12,5))
sns.barplot(x=tfidf_mean.values, y=tfidf_mean.index)
plt.title("Top 20 Important Words Based on TF-IDF")
plt.xlabel("TF-IDF Score")
plt.ylabel("Words")
plt.show()

"""# ðŸ“Œ Word2Vec Word Embeddings"""

sentences = [text.split() for text in df['clean_text']]
word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=2, workers=4)

# Get Vocabulary
words = list(word2vec_model.wv.index_to_key)

# Reduce Word Vectors using t-SNE
word_vectors = np.array([word2vec_model.wv[w] for w in words[:200]])  # Limit to 200 words for visualization
tsne = TSNE(n_components=2, perplexity=30, random_state=42)
word_vecs_2d = tsne.fit_transform(word_vectors)

# Plot Word Embeddings
plt.figure(figsize=(12, 6))
plt.scatter(word_vecs_2d[:, 0], word_vecs_2d[:, 1], marker='o', alpha=0.7)

# Add Word Labels
for i, word in enumerate(words[:200]):
    plt.annotate(word, xy=(word_vecs_2d[i, 0], word_vecs_2d[i, 1]), fontsize=9, alpha=0.7)

plt.title("Word2Vec Word Embeddings Visualization using t-SNE")
plt.show()

"""# ðŸ“Œ BERT Embeddings & PCA Visualization"""

from transformers import BertTokenizer, BertModel
import torch

# Load BERT tokenizer and model
bert_tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
bert_model = BertModel.from_pretrained("bert-base-uncased")

# Function to get BERT embeddings
def get_bert_embeddings(text):
    inputs = bert_tokenizer(text, return_tensors="pt", truncation=True, padding="max_length", max_length=30)
    with torch.no_grad():
        outputs = bert_model(**inputs)
    return outputs.last_hidden_state[:,0,:].numpy().flatten()

# Apply BERT embeddings (Limit to first 500 rows for performance)
df_sample = df.iloc[:500]
df_sample['bert_embedding'] = df_sample['clean_text'].apply(get_bert_embeddings)

# Convert embeddings to DataFrame
bert_embeddings = np.stack(df_sample['bert_embedding'].values)

# Reduce Dimensions using PCA
pca = PCA(n_components=2)
bert_pca = pca.fit_transform(bert_embeddings)

# Scatter plot for BERT embeddings
plt.figure(figsize=(10,6))
plt.scatter(bert_pca[:,0], bert_pca[:,1], alpha=0.5)
plt.title("BERT Embeddings Reduced using PCA")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.show()